**MaskRCNN三点主要区别**：

1. ResNet-FPN网络结构： 多层特征图有利于多尺度物体及小物体的检测；
2. RoI Align：RoI Pooling的取整做法损失了一些精度；RoI Align的思想是使用双线性插值获得坐标为浮点数的点的值；
3. Mask分支：预测每一个像素的类别。





**重点参考：https://github.com/facebookresearch/maskrcnn-benchmark**

+ 先在maskrcnn-benchmark的backbone中添加hrnet（Done）

+ 如何在maskrcnn的关键点检测方法中引入hrnet中的heatmap呢？结合hrnet中的lib/dataset/中的coco.py和JointsDataset.py，重写maskrcnn-benchmark中的data/datasets/中的coco.py。

  + 注意，hrnet中dataloader的dataset类的get item是加载人的实例，而maskrcnn训练时get item加载的是图片。
  + 要注意keypoint_head的输出要从[N, K, 3]变换成[N, K, 64, 48]，并重新计算损失函数。原损失函数在modeling/roi_heads/keypoint_head/loss.py中。

+ 要适配HR_MaskRCNN关键点检测任务，需要修改configs中的yaml配置文件

  

+ 后面持续将DARK、UDP、AID的思想引入进来。**当前先完成heatmap的替换**。**吃透吃透吃透**！

+ 后续再结合APDI对RPN进行改进，整体优化。

+ 基于detectron2完成hrnet的关键点检测。参考centermask2。



2021/02/03完成可视化

### roi_head

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
import torch

from .box_head.box_head import build_roi_box_head
from .mask_head.mask_head import build_roi_mask_head
from .keypoint_head.keypoint_head import build_roi_keypoint_head


class CombinedROIHeads(torch.nn.ModuleDict):
    """
    Combines a set of individual heads (for box prediction or masks) into a single head.
    """

    def __init__(self, cfg, heads):
        super(CombinedROIHeads, self).__init__(heads)
        self.cfg = cfg.clone()
        if cfg.MODEL.MASK_ON and cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR:
            self.mask.feature_extractor = self.box.feature_extractor
        if cfg.MODEL.KEYPOINT_ON and cfg.MODEL.ROI_KEYPOINT_HEAD.SHARE_BOX_FEATURE_EXTRACTOR:
            self.keypoint.feature_extractor = self.box.feature_extractor

    def forward(self, features, proposals, targets=None):
        losses = {}
        # TODO rename x to roi_box_features, if it doesn't increase memory consumption
        x, detections, loss_box = self.box(features, proposals, targets)
        losses.update(loss_box)
        if self.cfg.MODEL.MASK_ON:
            mask_features = features
            # optimization: during training, if we share the feature extractor between
            # the box and the mask heads, then we can reuse the features already computed
            if (
                self.training
                and self.cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR
            ):
                mask_features = x
            # During training, self.box() will return the unaltered proposals as "detections"
            # this makes the API consistent during training and testing
            x, detections, loss_mask = self.mask(mask_features, detections, targets)
            losses.update(loss_mask)

        if self.cfg.MODEL.KEYPOINT_ON:
            keypoint_features = features
            # optimization: during training, if we share the feature extractor between
            # the box and the mask heads, then we can reuse the features already computed
            if (
                self.training
                and self.cfg.MODEL.ROI_KEYPOINT_HEAD.SHARE_BOX_FEATURE_EXTRACTOR
            ):
                keypoint_features = x
            # During training, self.box() will return the unaltered proposals as "detections"
            # this makes the API consistent during training and testing
            x, detections, loss_keypoint = self.keypoint(keypoint_features, detections, targets)
            losses.update(loss_keypoint)
        return x, detections, losses


def build_roi_heads(cfg, in_channels):
    # individually create the heads, that will be combined together afterwards
    roi_heads = []
    if cfg.MODEL.RETINANET_ON:
        return []

    if not cfg.MODEL.RPN_ONLY:
        roi_heads.append(("box", build_roi_box_head(cfg, in_channels)))
    if cfg.MODEL.MASK_ON:
        roi_heads.append(("mask", build_roi_mask_head(cfg, in_channels)))
    if cfg.MODEL.KEYPOINT_ON:
        roi_heads.append(("keypoint", build_roi_keypoint_head(cfg, in_channels)))

    # combine individual heads in a single module
    if roi_heads:
        roi_heads = CombinedROIHeads(cfg, roi_heads)

    return roi_heads

```



### keypoint_head

+ [maskrcnn_benchmark/modeling/roi_heads/keypoint_head/keypoint_head.py](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/keypoint_head.py) (也可参考[facebookresearch/detectron2/modeling/roi_heads/keypoint_head.py](https://github.com/facebookresearch/detectron2))。

```python
import torch

from .roi_keypoint_feature_extractors import make_roi_keypoint_feature_extractor
from .roi_keypoint_predictors import make_roi_keypoint_predictor
from .inference import make_roi_keypoint_post_processor
from .loss import make_roi_keypoint_loss_evaluator


class ROIKeypointHead(torch.nn.Module):
    def __init__(self, cfg, in_channels):
        super(ROIKeypointHead, self).__init__()
        self.cfg = cfg.clone()
        self.feature_extractor = make_roi_keypoint_feature_extractor(cfg, in_channels)
        self.predictor = make_roi_keypoint_predictor(
            cfg, self.feature_extractor.out_channels)
        self.post_processor = make_roi_keypoint_post_processor(cfg)
        self.loss_evaluator = make_roi_keypoint_loss_evaluator(cfg)

    def forward(self, features, proposals, targets=None):
        """
        Arguments:
            features (list[Tensor]): feature-maps from possibly several levels
            proposals (list[BoxList]): proposal boxes
            targets (list[BoxList], optional): the ground-truth targets.

        Returns:
            x (Tensor): the result of the feature extractor
            proposals (list[BoxList]): during training, the original proposals
                are returned. During testing, the predicted boxlists are returned
                with the `mask` field set
            losses (dict[Tensor]): During training, returns the losses for the
                head. During testing, returns an empty dict.
        """
        if self.training:
            with torch.no_grad():
                proposals = self.loss_evaluator.subsample(proposals, targets)

        x = self.feature_extractor(features, proposals)
        kp_logits = self.predictor(x)

        if not self.training:
            result = self.post_processor(kp_logits, proposals)
            return x, result, {}

        loss_kp = self.loss_evaluator(proposals, kp_logits)

        return x, proposals, dict(loss_kp=loss_kp)


def build_roi_keypoint_head(cfg, in_channels):
    return ROIKeypointHead(cfg, in_channels)

```



+ 修改`maskrcnn_benchmark/modeling/detector/generalized_rcnn.py`

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
"""
Implements the Generalized R-CNN framework
"""

import torch
from torch import nn

from maskrcnn_benchmark.structures.image_list import to_image_list

from ..backbone import build_backbone
from ..rpn.rpn import build_rpn
from ..roi_heads.roi_heads import build_roi_heads


class GeneralizedRCNN(nn.Module):
    """
    Main class for Generalized R-CNN. Currently supports boxes and masks.
    It consists of three main parts:
    - backbone
    = rpn
    - heads: takes the features + the proposals from the RPN and computes
        detections / masks from it.
    """

    def __init__(self, cfg):
        super(GeneralizedRCNN, self).__init__()

        self.backbone = build_backbone(cfg)
        self.rpn = build_rpn(cfg)
        self.roi_heads = build_roi_heads(cfg)

    def forward(self, images, targets=None):
        """
        Arguments:
            images (list[Tensor] or ImageList): images to be processed
            targets (list[BoxList]): ground-truth boxes present in the image (optional)

        Returns:
            result (list[BoxList] or dict[Tensor]): the output from the model.
                During training, it returns a dict[Tensor] which contains the losses.
                During testing, it returns list[BoxList] contains additional fields
                like `scores`, `labels` and `mask` (for Mask R-CNN models).

        """
        if self.training and targets is None:
            raise ValueError("In training mode, targets should be passed")
        images = to_image_list(images)
        features = self.backbone(images.tensors)
        proposals, proposal_losses = self.rpn(images, features, targets)
        if self.roi_heads:
            x, result, detector_losses = self.roi_heads(features, proposals, targets)
        else:
            # RPN-only models don't have roi_heads
            x = features
            result = proposals
            detector_losses = {}

        if self.training:
            losses = {}
            losses.update(detector_losses)
            losses.update(proposal_losses)
            return losses

        return result

```



### backbone

创建骨干网络：`maskrcnn_benchmark/modeling/backbone/backbone.py`；

```python
def build_backbone(cfg):
    assert cfg.MODEL.BACKBONE.CONV_BODY in registry.BACKBONES, \
        "cfg.MODEL.BACKBONE.CONV_BODY: {} are not registered in registry".format(
            cfg.MODEL.BACKBONE.CONV_BODY
        )
    return registry.BACKBONES[cfg.MODEL.BACKBONE.CONV_BODY](cfg)
```



HRNET-HR(我觉得应该叫HRNET-FPN)骨干网络结构：`maskrcnn_benchmark/modeling/backbone/backbone.py`；

```python
@registry.BACKBONES.register('HRNET-HR')
def build_hrnet_hr_backbone(cfg):
    body = hrnet.HRNet(cfg)
    neck = hrfpn.HRFPN(cfg)
    model = nn.Sequential(OrderedDict([('body', body), ('neck', neck)]))
    return model
```



网络结构HRFPN：`maskrcnn_benchmark/modeling/backbone/hrfpn.py`；

```python
class HRFPN(nn.Module):

    def __init__(self, cfg):
        super(HRFPN, self).__init__()

        config = cfg.MODEL.NECK
```



```yaml
MODEL:  
  NECK:
    IN_CHANNELS:
      - 18
      - 36
      - 72
      - 144
    OUT_CHANNELS: 256
    POOLING: "AVG"
```





### rpn





### head









+ 关注一下[vision/torchvision/models/detection/roi_heads.py](https://github.com/pytorch/vision/blob/f7fae490980885e426fef01bb214025b9eddb832/torchvision/models/detection/roi_heads.py)中的select_training_samples中的regression_targets；（讲gt_bbox加入到proposals？为什么呢？让网络更好训练吗？刚开始的时候，proposals可能不准，不合适的框用来做关键点检测可能没啥用）







## Debug

#### 数据集错误

```shell
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/config/paths_catalog.py", line 156, in get
    attrs = DatasetCatalog.DATASETS[name]
KeyError: 'keypoints_coco_2017_train'
```

修改配置文件：maskrcnn-benchmark/configs/e2e_keypoint_rcnn_R_50_FPN_1x_train2017.yaml

```yaml
DATASETS:
  TRAIN: ("keypoints_coco_2017_train", "keypoints_coco_2017_val",)
  TEST: ("keypoints_coco_2017_val",)
```

修改paths：maskrcnn-benchmark/maskrcnn_benchmark/config/paths_catalog.py

```python
        "keypoints_coco_2017_train": {
            "img_dir": "coco/images/train2017",
            "ann_file": "coco/annotations/person_keypoints_train2017.json",
        },
        "keypoints_coco_2017_val": {
            "img_dir": "coco/images/val2017",
            "ann_file": "coco/annotations/person_keypoints_val2017.json"
        },
```

#### 显存不足

```shell
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 7.80 GiB total capacity; 6.58 GiB already allocated; 90.31 MiB free; 7.01 GiB reserved in total by PyTorch)
```

默认是16：/maskrcnn_benchmark/config/defaults.py

```python
# Number of images per batch
# This is global, so if we have 8 GPUs and IMS_PER_BATCH = 16, each GPU will
# see 2 images per batch
_C.SOLVER.IMS_PER_BATCH = 16
```

修改配置文件：maskrcnn-benchmark/configs/e2e_keypoint_rcnn_R_50_FPN_1x_train2017.yaml

```yaml
SOLVER:
  IMS_PER_BATCH: 2
```



#### "index out of bounds"` failed

#### device-side assert triggered

```shell
RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered
```

参考：https://github.com/facebookresearch/maskrcnn-benchmark/issues/733

```txt
Jayis commented on 30 May 2019
same error

if the problem happened in training, smaller learning rate helps
but I've also encountered this error while testing once...
while testing, that's not possible to solve it by using smaller learning rate, right?

I've tried to debug, but I can't even access the "boxlist".
Runtime error happened when I try to print the "boxlist".

I really want to know if anybody had another solution rather than just "using smaller learning rate"...
```

把学习率调小：

```yaml
SOLVER:
  BASE_LR: 0.0005
  WEIGHT_DECAY: 0.00001
```



#### 'Non-existent config key: MODEL.BACKBONE.OUT_CHANNELS'

```shell
  File "tools/train_net.py", line 172, in main
    cfg.merge_from_file(args.config_file)
```


在默认的config文件中添加MODEL.BACKBONE.OUT_CHANNELS：/maskrcnn_benchmark/config/defaults.py

```yaml
_C.MODEL.BACKBONE.OUT_CHANNELS = 256 * 4
```



#### 'Sequential' object has no attribute 'out_channels'

```shell
  File "/home/wenxue/projects/maskrcnn-benchmark/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py", line 30, in __init__
    self.rpn = build_rpn(cfg, self.backbone.out_channels)  ##self.rpn为build_rpn中的RPNModule(cfg, in_channels)

```

对比maskrcnn-benchmark与HRNet_MaskRCNN中的/maskrcnn_benchmark/modeling/rpn/rpn.py，前者带参数in_channels，后者不带；后者在`class RPNModule(torch.nn.Module)`中多了一行代码`in_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS`；因此我的修改方案是在/maskrcnn_benchmark/modeling/backbone/backbone.py中对hrnet进行修改，添加out_channels属性：

```python
@registry.BACKBONES.register("HRNET")
def build_hrnet_backbone(cfg):
    body = hrnet.HRNet(cfg)
    model = nn.Sequential(OrderedDict([("body", body)]))
    model.out_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS
    return model


@registry.BACKBONES.register('HRNET-FPN')  ##我觉得应该叫HRNET-FPN
def build_hrnet_fpn_backbone(cfg):
    body = hrnet.HRNet(cfg)
    neck = hrfpn.HRFPN(cfg)
    model = nn.Sequential(OrderedDict([('body', body), ('neck', neck)]))
    model.out_channels = cfg.MODEL.BACKBONE.OUT_CHANNELS
    return model
```



根据HRNet_MaskRCNN进行修改：/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py

注意在maskrcnn-benchmark中的/maskrcnn_benchmark/modeling/backbone/backbone.py中，resnet等模型都有out_channels这个属性；

```python
@registry.BACKBONES.register("R-50-C4")
@registry.BACKBONES.register("R-50-C5")
@registry.BACKBONES.register("R-101-C4")
@registry.BACKBONES.register("R-101-C5")
def build_resnet_backbone(cfg):
    body = resnet.ResNet(cfg)
    model = nn.Sequential(OrderedDict([("body", body)]))
    model.out_channels = cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS
    return model


@registry.BACKBONES.register("R-50-FPN")
@registry.BACKBONES.register("R-101-FPN")
@registry.BACKBONES.register("R-152-FPN")
def build_resnet_fpn_backbone(cfg):
    body = resnet.ResNet(cfg)
    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    out_channels = cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS
    fpn = fpn_module.FPN(
        in_channels_list=[
            in_channels_stage2,
            in_channels_stage2 * 2,
            in_channels_stage2 * 4,
            in_channels_stage2 * 8,
        ],
        out_channels=out_channels,
        conv_block=conv_with_kaiming_uniform(
            cfg.MODEL.FPN.USE_GN, cfg.MODEL.FPN.USE_RELU
        ),
        top_blocks=fpn_module.LastLevelMaxPool(),
    )
    model = nn.Sequential(OrderedDict([("body", body), ("fpn", fpn)]))
    model.out_channels = out_channels
    return model


@registry.BACKBONES.register("R-50-FPN-RETINANET")
@registry.BACKBONES.register("R-101-FPN-RETINANET")
def build_resnet_fpn_p3p7_backbone(cfg):
    body = resnet.ResNet(cfg)
    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    out_channels = cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS
    in_channels_p6p7 = in_channels_stage2 * 8 if cfg.MODEL.RETINANET.USE_C5 \
        else out_channels
    fpn = fpn_module.FPN(
        in_channels_list=[
            0,
            in_channels_stage2 * 2,
            in_channels_stage2 * 4,
            in_channels_stage2 * 8,
        ],
        out_channels=out_channels,
        conv_block=conv_with_kaiming_uniform(
            cfg.MODEL.FPN.USE_GN, cfg.MODEL.FPN.USE_RELU
        ),
        top_blocks=fpn_module.LastLevelP6P7(in_channels_p6p7, out_channels),
    )
    model = nn.Sequential(OrderedDict([("body", body), ("fpn", fpn)]))
    model.out_channels = out_channels
    return model
```

而HRNet_MaskRCNN中的hrnet是没有这个属性的。因此解决办法是：搞清楚HRNet_MaskRCNN中的hrnet的out_channels属性在传参时候的异同，进行修改。

```python
@registry.BACKBONES.register("HRNET")
def build_msnet_backbone(cfg):
    body = hrnet.HRNet(cfg)
    model = nn.Sequential(OrderedDict([("body", body)]))
    return model


@registry.BACKBONES.register('HRNET-FPN')  ##我觉得应该叫HRNET-FPN
def build_hrnet_hr_backbone(cfg):
    body = hrnet.HRNet(cfg)
    neck = hrfpn.HRFPN(cfg)
    model = nn.Sequential(OrderedDict([('body', body), ('neck', neck)]))
    return model
```





在maskrcnn的关键点检测方法中引入hrnet中的heatmap，结合hrnet中的lib/dataset/中的coco.py和JointsDataset.py，重写maskrcnn-benchmark中的/maskrcnn_benchmark/data/datasets/coco.py

```python
    def __getitem__(self, idx):
        img, anno = super(COCODataset, self).__getitem__(idx)

        # filter crowd annotations
        # TODO might be better to add an extra field
        anno = [obj for obj in anno if obj["iscrowd"] == 0]

        boxes = [obj["bbox"] for obj in anno]
        boxes = torch.as_tensor(boxes).reshape(-1, 4)  # guard against no boxes
        target = BoxList(boxes, img.size, mode="xywh").convert("xyxy")

        classes = [obj["category_id"] for obj in anno]
        classes = [self.json_category_id_to_contiguous_id[c] for c in classes]
        classes = torch.tensor(classes)
        target.add_field("labels", classes)

        if anno and "segmentation" in anno[0]:
            masks = [obj["segmentation"] for obj in anno]
            masks = SegmentationMask(masks, img.size, mode='poly')
            target.add_field("masks", masks)

        if anno and "keypoints" in anno[0]:
            keypoints = [obj["keypoints"] for obj in anno]
            keypoints = PersonKeypoints(keypoints, img.size)
            target.add_field("keypoints", keypoints)

        target = target.clip_to_image(remove_empty=True)

        if self._transforms is not None:
            img, target = self._transforms(img, target)

        return img, target, idx
```

+ **把上面的target重写好**：



+ 注意，hrnet中dataloader的dataset类的get item是加载人的实例，而maskrcnn训练时get item加载的是图片。
+ 要注意keypoint_head的输出要从[N, K, 3]变换成[N, K, 64, 48]，并重新计算损失函数。原损失函数在modeling/roi_heads/keypoint_head/loss.py中。





训练过程中损失函数的调用：

```python
    for iteration, (images, targets, _) in enumerate(data_loader, start_iter):
        
        if any(len(target) < 1 for target in targets):
            logger.error(f"Iteration={iteration + 1} || Image Ids used for training {_} || targets Length={[len(target) for target in targets]}" )
            continue
        data_time = time.time() - end
        iteration = iteration + 1
        arguments["iteration"] = iteration

        images = images.to(device)
        targets = [target.to(device) for target in targets]

        loss_dict = model(images, targets)

        losses = sum(loss for loss in loss_dict.values())
```

因此，当前的重点是把targets和loss重写好。

以终为始，先看loss。/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/keypoint_head.py中会求取损失函数并返回：

```python
class ROIKeypointHead(torch.nn.Module):
    def __init__(self, cfg, in_channels):
        super(ROIKeypointHead, self).__init__()
        self.cfg = cfg.clone()
        self.feature_extractor = make_roi_keypoint_feature_extractor(cfg, in_channels)
        self.predictor = make_roi_keypoint_predictor(
            cfg, self.feature_extractor.out_channels)
        self.post_processor = make_roi_keypoint_post_processor(cfg)
        self.loss_evaluator = make_roi_keypoint_loss_evaluator(cfg)

    def forward(self, features, proposals, targets=None):
        """
        Arguments:
            features (list[Tensor]): feature-maps from possibly several levels
            proposals (list[BoxList]): proposal boxes
            targets (list[BoxList], optional): the ground-truth targets.

        Returns:
            x (Tensor): the result of the feature extractor
            proposals (list[BoxList]): during training, the original proposals
                are returned. During testing, the predicted boxlists are returned
                with the `mask` field set
            losses (dict[Tensor]): During training, returns the losses for the
                head. During testing, returns an empty dict.
        """
        if self.training:
            with torch.no_grad():
                proposals = self.loss_evaluator.subsample(proposals, targets)  ##调用的是loss.py中的class KeypointRCNNLossComputation(object):中的subsample

        x = self.feature_extractor(features, proposals)  ##此处得到的是[persom_num, 17, 28, 28]的特征图，此处修改成heatmap的维度
        kp_logits = self.predictor(x)  ##得到关键点的坐标[persom_num, 17, 3]，这一层直接干掉，将上面的heatmap与gt heatmap一起用于计算损失函数

        if not self.training:
            result = self.post_processor(kp_logits, proposals)
            return x, result, {}

        loss_kp = self.loss_evaluator(proposals, kp_logits)

        return x, proposals, dict(loss_kp=loss_kp)
        
        
```

keypoint损失函数计算：/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/loss.py中在计算loss

```python
def make_roi_keypoint_loss_evaluator(cfg):
    matcher = Matcher(
        cfg.MODEL.ROI_HEADS.FG_IOU_THRESHOLD,
        cfg.MODEL.ROI_HEADS.BG_IOU_THRESHOLD,
        allow_low_quality_matches=False,
    )
    fg_bg_sampler = BalancedPositiveNegativeSampler(
        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE, cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION
    )
    resolution = cfg.MODEL.ROI_KEYPOINT_HEAD.RESOLUTION  ##在yaml配置文件中赋值了，为56。
    loss_evaluator = KeypointRCNNLossComputation(matcher, fg_bg_sampler, resolution)
    return loss_evaluator

```

```python
class KeypointRCNNLossComputation(object):
    def __init__(self, proposal_matcher, fg_bg_sampler, discretization_size):
        """
        Arguments:
            proposal_matcher (Matcher)
            fg_bg_sampler (BalancedPositiveNegativeSampler)
            discretization_size (int)
        """
        self.proposal_matcher = proposal_matcher
        self.fg_bg_sampler = fg_bg_sampler
        self.discretization_size = discretization_size

    def __call__(self, proposals, keypoint_logits):
        heatmaps = []
        valid = []
        for proposals_per_image in proposals:
            kp = proposals_per_image.get_field("keypoints")
            heatmaps_per_image, valid_per_image = project_keypoints_to_heatmap(
                kp, proposals_per_image, self.discretization_size
            )
            heatmaps.append(heatmaps_per_image.view(-1))
            valid.append(valid_per_image.view(-1))

        keypoint_targets = cat(heatmaps, dim=0)
        valid = cat(valid, dim=0).to(dtype=torch.bool)
        valid = torch.nonzero(valid).squeeze(1)

        # torch.mean (in binary_cross_entropy_with_logits) does'nt
        # accept empty tensors, so handle it separately
        if keypoint_targets.numel() == 0 or len(valid) == 0:
            return keypoint_logits.sum() * 0

        N, K, H, W = keypoint_logits.shape
        keypoint_logits = keypoint_logits.view(N * K, H * W)

        keypoint_loss = F.cross_entropy(keypoint_logits[valid], keypoint_targets[valid])
        return keypoint_loss
```

```python
def project_keypoints_to_heatmap(keypoints, proposals, discretization_size):
    proposals = proposals.convert("xyxy")
    return keypoints_to_heat_map(
        keypoints.keypoints, proposals.bbox, discretization_size
    )
```

调用了/maskrcnn_benchmark/structures/keypoint.py中的keypoints_to_heat_map：

```python
# TODO make this nicer, this is a direct translation from C2 (but removing the inner loop)
def keypoints_to_heat_map(keypoints, rois, heatmap_size):
    if rois.numel() == 0:
        return rois.new().long(), rois.new().long()
    offset_x = rois[:, 0]
    offset_y = rois[:, 1]
    scale_x = heatmap_size / (rois[:, 2] - rois[:, 0])
    scale_y = heatmap_size / (rois[:, 3] - rois[:, 1])

    offset_x = offset_x[:, None]
    offset_y = offset_y[:, None]
    scale_x = scale_x[:, None]
    scale_y = scale_y[:, None]

    x = keypoints[..., 0]
    y = keypoints[..., 1]

    x_boundary_inds = x == rois[:, 2][:, None]
    y_boundary_inds = y == rois[:, 3][:, None]

    x = (x - offset_x) * scale_x
    x = x.floor().long()
    y = (y - offset_y) * scale_y
    y = y.floor().long()
    
    x[x_boundary_inds] = heatmap_size - 1
    y[y_boundary_inds] = heatmap_size - 1

    valid_loc = (x >= 0) & (y >= 0) & (x < heatmap_size) & (y < heatmap_size)
    vis = keypoints[..., 2] > 0
    valid = (valid_loc & vis).long()

    lin_ind = y * heatmap_size + x
    heatmaps = lin_ind * valid

    ##heatmaps的维度为[person_num, 17, 3], valid的维度为[person_num, 17]
    return heatmaps, valid
```





keypoint_head中预测关键点的地方：

```python
@registry.ROI_KEYPOINT_PREDICTOR.register("KeypointRCNNPredictor")
class KeypointRCNNPredictor(nn.Module):
    def __init__(self, cfg, in_channels):
        super(KeypointRCNNPredictor, self).__init__()
        input_features = in_channels
        num_keypoints = cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_CLASSES
        deconv_kernel = 4
        self.kps_score_lowres = layers.ConvTranspose2d(
            input_features,
            num_keypoints,
            deconv_kernel,
            stride=2,
            padding=deconv_kernel // 2 - 1,
        )
        nn.init.kaiming_normal_(
            self.kps_score_lowres.weight, mode="fan_out", nonlinearity="relu"
        )
        nn.init.constant_(self.kps_score_lowres.bias, 0)
        self.up_scale = 2
        self.out_channels = num_keypoints

    def forward(self, x):
        x = self.kps_score_lowres(x)
        x = layers.interpolate(
            x, scale_factor=self.up_scale, mode="bilinear", align_corners=False
        )
        return x


def make_roi_keypoint_predictor(cfg, in_channels):
    func = registry.ROI_KEYPOINT_PREDICTOR[cfg.MODEL.ROI_KEYPOINT_HEAD.PREDICTOR]
    return func(cfg, in_channels)
```





/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/roi_keypoint_feature_extractors.py

```python
from torch import nn
from torch.nn import functional as F

from maskrcnn_benchmark.modeling import registry
from maskrcnn_benchmark.modeling.poolers import Pooler

from maskrcnn_benchmark.layers import Conv2d


@registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register("KeypointRCNNFeatureExtractor")
class KeypointRCNNFeatureExtractor(nn.Module):
    def __init__(self, cfg, in_channels):
        super(KeypointRCNNFeatureExtractor, self).__init__()

        resolution = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_RESOLUTION
        scales = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_SCALES
        sampling_ratio = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_SAMPLING_RATIO
        pooler = Pooler(
            output_size=(resolution, resolution),
            scales=scales,
            sampling_ratio=sampling_ratio,
        )
        self.pooler = pooler

        input_features = in_channels
        layers = cfg.MODEL.ROI_KEYPOINT_HEAD.CONV_LAYERS  ##在maskrcnn_benchmark/config/defaults.py中
        next_feature = input_features
        self.blocks = []
        for layer_idx, layer_features in enumerate(layers, 1):
            layer_name = "conv_fcn{}".format(layer_idx)
            module = Conv2d(next_feature, layer_features, 3, stride=1, padding=1)
            nn.init.kaiming_normal_(module.weight, mode="fan_out", nonlinearity="relu")
            nn.init.constant_(module.bias, 0)
            self.add_module(layer_name, module)
            next_feature = layer_features
            self.blocks.append(layer_name)
        self.out_channels = layer_features

    def forward(self, x, proposals):
        x = self.pooler(x, proposals)
        for layer_name in self.blocks:
            x = F.relu(getattr(self, layer_name)(x))
        return x


def make_roi_keypoint_feature_extractor(cfg, in_channels):
    func = registry.ROI_KEYPOINT_FEATURE_EXTRACTORS[
        cfg.MODEL.ROI_KEYPOINT_HEAD.FEATURE_EXTRACTOR
    ]
    return func(cfg, in_channels)

```

在maskrcnn_benchmark/config/defaults.py中

```python
_C.MODEL.ROI_KEYPOINT_HEAD = CN()
_C.MODEL.ROI_KEYPOINT_HEAD.FEATURE_EXTRACTOR = "KeypointRCNNFeatureExtractor"
_C.MODEL.ROI_KEYPOINT_HEAD.PREDICTOR = "KeypointRCNNPredictor"
_C.MODEL.ROI_KEYPOINT_HEAD.POOLER_RESOLUTION = 14
_C.MODEL.ROI_KEYPOINT_HEAD.POOLER_SAMPLING_RATIO = 0
_C.MODEL.ROI_KEYPOINT_HEAD.POOLER_SCALES = (1.0 / 16,)
_C.MODEL.ROI_KEYPOINT_HEAD.MLP_HEAD_DIM = 1024
_C.MODEL.ROI_KEYPOINT_HEAD.CONV_LAYERS = tuple(512 for _ in range(8))
_C.MODEL.ROI_KEYPOINT_HEAD.RESOLUTION = 14
_C.MODEL.ROI_KEYPOINT_HEAD.NUM_CLASSES = 17
_C.MODEL.ROI_KEYPOINT_HEAD.SHARE_BOX_FEATURE_EXTRACTOR = True
```



20210202

+ 换成heatmap的做法是把/maskrcnn_benchmark/structures/keypoint.py中的keypoints_to_heat_map重写，然后把/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/loss.py中计算loss的`class KeypointRCNNLossComputation(object):`中的`def __call__(self, proposals, keypoint_logits):`中的`keypoint_loss = F.cross_entropy(keypoint_logits[valid], keypoint_targets[valid])`换掉：

```python
keypoint_loss = torch.mean(torch.square(keypoint_logits[valid] - keypoint_targets[valid])) * 40
```



+ 













20210201训练完模型后，inference报错：

```txt
021-02-01 23:54:39,446 maskrcnn_benchmark.inference INFO: Start evaluation on keypoints_coco_2017_val dataset(5000 images).
/opt/Software/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode))
  0%|          | 0/626 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "tools/train_net.py", line 205, in <module>
    main()
  File "tools/train_net.py", line 201, in main
    run_test(cfg, model, args.distributed)
  File "tools/train_net.py", line 132, in run_test
    output_folder=output_folder,
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/engine/inference.py", line 84, in inference
    predictions = compute_on_dataset(model, data_loader, device, bbox_aug, inference_timer)
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/engine/inference.py", line 29, in compute_on_dataset
    output = model(images.to(device))
  File "/opt/Software/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/Software/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/apex-0.1-py3.6.egg/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py", line 56, in forward
    x, result, detector_losses = self.roi_heads(features, proposals, targets)
  File "/opt/Software/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/modeling/roi_heads/roi_heads.py", line 61, in forward
    x, detections, loss_keypoint = self.keypoint(keypoint_features, detections, targets)
  File "/opt/Software/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/keypoint_head.py", line 42, in forward
    result = self.post_processor(kp_logits, proposals)
  File "/opt/Software/miniconda3/envs/maskrcnn/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/inference.py", line 15, in forward
    mask_prob, scores = self.keypointer(x, boxes)
  File "/opt/SRC/projects/keypoint_detection/maskrcnn-benchmark/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/inference.py", line 114, in __call__
    assert len(boxes) == 1
AssertionError
```

解决办法是将测试时的IMS_PER_BATCH设为1

```yaml
TEST:
  IMS_PER_BATCH: 1
```





自己运行test_net.py后，得到如下结果：backbone换成hrnet后效果比之前的resnet好一些。

```
Evaluate annotation type *bbox*
DONE (t=29.05s).
Accumulating evaluation results...
DONE (t=2.55s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.585
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.181
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.375
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.119
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.358
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.516
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.551
Loading and preparing results...
DONE (t=11.84s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *keypoints*
DONE (t=21.38s).
Accumulating evaluation results...
DONE (t=0.68s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.466
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.685
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.495
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.437
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.527
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.604
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.832
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.632
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.542
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.691
2021-02-02 09:22:20,406 maskrcnn_benchmark.inference INFO: 
Task: bbox
AP, AP50, AP75, APs, APm, APl
0.2837, 0.5847, 0.2347, 0.1808, 0.3746, 0.3701
Task: keypoints
AP, AP50, AP75, APm, APl
0.4657, 0.6848, 0.4952, 0.4373, 0.5274

```







#### TypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

解决办法是先转换到cpu上，然后转换成list，再转换成np矩阵：

```python
        joints_3d =np.array(keypoints[k].cpu().tolist())  ##17代表关键点个数，3代表(x, y, v)，v为{0:不存在, 1:存在但不可见, 2:存在且可见}
        x1, y1, x2, y2 = list(map(int, np.array(rois[k].cpu().tolist())))
```

然后再转换成CUDA Tensor：

```python
heatmaps = torch.from_numpy(np.array(heatmaps)).cuda()
```





换成heatmap后，AP/AR很差，inference的时候发现是**logits的问题**：

```
ipdb> scores = keypoints.get_field("logits")
ipdb> scores
tensor([[0.6955, 0.7483, 0.8024, 0.8331, 0.8869, 0.7017, 0.7316, 0.4262, 0.4529,
         0.3222, 0.3678, 0.3645, 0.3384, 0.2215, 0.2590, 0.3021, 0.3240]])
```

而maskrcnn_hrnet模型的logits如下：

```
ipdb> scores = keypoints.get_field("logits")
ipdb> scores
tensor([[39.3170, 48.0535, 39.5069, 35.2508, 22.4835, 36.1933, 36.9994, 25.5425,
         29.9332, 26.2346, 32.1550, 47.2982, 41.0785, 23.6864, 25.6383, 10.4415,
         10.9849]])
```



```
ipdb> type(predictions)
<class 'list'>
ipdb> len(predictions)
1
ipdb> type(prediction)
<class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
ipdb> prediction.has_field("scores")
True
ipdb> scores = prediction.get_field("scores")
ipdb> scores
tensor([0.9502, 0.9259, 0.2231, 0.4365, 0.9589, 0.6076, 0.3360, 0.2190, 0.2771,
        0.1377, 0.4238, 0.1020, 0.5172, 0.0706, 0.3602, 0.4077, 0.0787, 0.5043,
        0.2913, 0.0763, 0.0705, 0.3390, 0.1031, 0.0556, 0.1637, 0.0782, 0.1316,
        0.1271, 0.1084, 0.1732, 0.2038, 0.9478, 0.0585, 0.0862, 0.0909, 0.5879,
        0.0685, 0.3617, 0.2947, 0.0920, 0.6633, 0.1492, 0.0637, 0.0923, 0.0577,
        0.1080, 0.0659, 0.2720, 0.0800, 0.2158, 0.1674, 0.0617, 0.2089, 0.0975])
ipdb> keep = torch.nonzero(scores > 0.7).squeeze(1)
ipdb> keep
tensor([ 0,  1,  4, 31])
ipdb> prediction = prediction[keep]
ipdb> scores = prediction.get_field("scores")
ipdb> scores
tensor([0.9502, 0.9259, 0.9589, 0.9478])
ipdb> _, idx = scores.sort(0, descending=True)
ipdb> predictions[idx]
*** TypeError: only integer tensors of a single element can be converted to an index
ipdb> idx
tensor([2, 0, 3, 1])
ipdb> prediction[idx]
BoxList(num_boxes=4, image_width=640, image_height=480, mode=xyxy)
ipdb> keypoints = prediction.get_field("keypoints")
ipdb> keypoints
PersonKeypoints(num_instances=4, image_width=640, image_height=480)
ipdb> kps = keypoints.keypoints
ipdb> kps
tensor([[[288.7888, 173.4450,   1.0000],
         [288.7888, 171.0497,   1.0000],
         [284.0178, 171.0497,   1.0000],
         [293.5598, 171.6485,   1.0000],
         [279.8431, 171.6485,   1.0000],
         [302.5054, 190.2116,   1.0000],
         [266.1265, 190.2116,   1.0000],
         [301.9091, 235.7211,   1.0000],
         [246.4460, 235.7211,   1.0000],
         [250.6207, 244.1044,   1.0000],
         [245.2533, 250.6913,   1.0000],
         [292.3671, 258.4759,   1.0000],
         [266.1265, 267.4580,   1.0000],
         [263.7410, 308.7758,   1.0000],
         [263.1446, 307.5782,   1.0000],
         [284.0178, 360.2734,   1.0000],
         [269.1084, 362.6686,   1.0000]],

        [[200.7783, 163.5871,   1.0000],
         [205.5641, 156.9902,   1.0000],
         [193.5996, 153.3919,   1.0000],
         [212.1446, 158.7893,   1.0000],
         [190.0102, 156.9902,   1.0000],
         [222.9127, 190.5741,   1.0000],
         [167.8759, 187.5756,   1.0000],
         [223.5109, 234.3532,   1.0000],
         [146.9379, 230.7549,   1.0000],
         [217.5286, 245.1480,   1.0000],
         [139.7592, 251.7449,   1.0000],
         [169.6705, 268.5369,   1.0000],
         [160.0989, 267.3374,   1.0000],
         [149.3308, 315.9142,   1.0000],
         [147.5362, 317.1136,   1.0000],
         [140.3575, 376.4852,   1.0000],
         [137.9645, 370.4881,   1.0000]],

        [[338.0375, 166.6352,   1.0000],
         [341.0226, 163.0421,   1.0000],
         [335.0525, 163.0421,   1.0000],
         [347.5898, 161.8443,   1.0000],
         [329.6794, 163.0421,   1.0000],
         [355.9479, 194.7813,   1.0000],
         [319.5302, 191.1882,   1.0000],
         [361.9181, 224.1251,   1.0000],
         [311.7690, 215.7412,   1.0000],
         [367.2912, 238.4976,   1.0000],
         [311.7690, 239.0965,   1.0000],
         [355.9479, 246.8816,   1.0000],
         [326.0973, 254.6667,   1.0000],
         [326.6943, 301.3773,   1.0000],
         [323.7093, 294.7899,   1.0000],
         [326.0973, 346.2913,   1.0000],
         [323.1122, 351.0821,   1.0000]],

        [[412.8731, 167.8467,   1.0000],
         [417.0666, 161.2653,   1.0000],
         [411.0759, 161.8636,   1.0000],
         [421.2600, 163.0603,   1.0000],
         [406.8825, 164.2569,   1.0000],
         [426.6516, 190.5825,   1.0000],
         [397.2975, 188.7876,   1.0000],
         [430.8450, 222.2929,   1.0000],
         [382.9200, 227.6777,   1.0000],
         [383.5191, 246.8236,   1.0000],
         [383.5191, 242.6354,   1.0000],
         [417.0666, 248.0202,   1.0000],
         [403.8872, 244.4304,   1.0000],
         [411.0759, 291.0985,   1.0000],
         [411.0759, 286.9103,   1.0000],
         [389.5097, 332.9802,   1.0000],
         [387.7125, 330.5869,   1.0000]]])
ipdb> type(kps)
<class 'torch.Tensor'>
ipdb> kps.shape
torch.Size([4, 17, 3])
ipdb> 
```





20210319：

**predictions：**

+ maskrcnn_benchmark/modeling/detector/generalized_rcnn.py

```python
class GeneralizedRCNN(nn.Module):
        if self.roi_heads:
            ##self.roi_heads为build_roi_heads中的CombinedROIHeads(cfg, roi_heads)
            ##CombinedROIHeads返回x, detections, losses
            x, result, detector_losses = self.roi_heads(features, proposals, targets)
        return result
```

+ maskrcnn_benchmark/modeling/roi_heads/roi_heads.py

```python
def build_roi_heads(cfg, in_channels):
    # individually create the heads, that will be combined together afterwards
    roi_heads = []
    if cfg.MODEL.RETINANET_ON:
        return []

    ##_C.MODEL.RPN_ONLY = False(maskrcnn_benchmark/config/defaults.py)
    if not cfg.MODEL.RPN_ONLY:
        roi_heads.append(("box", build_roi_box_head(cfg, in_channels)))
    if cfg.MODEL.MASK_ON:
        roi_heads.append(("mask", build_roi_mask_head(cfg, in_channels)))
    if cfg.MODEL.KEYPOINT_ON:
        roi_heads.append(("keypoint", build_roi_keypoint_head(cfg, in_channels)))

    # combine individual heads in a single module
    if roi_heads:
        roi_heads = CombinedROIHeads(cfg, roi_heads)

    return roi_heads
```

**关键点：**

+ maskrcnn_benchmark/modeling/roi_heads/keypoint_head/keypoint_head.py

```python
		kp_logits = self.predictor(x)  ##得到关键点的特征图[persom_num, 17, 56, 56]，将这一层与gt heatmap一起用于计算损失函数
		if not self.training:
            result = self.post_processor(kp_logits, proposals)
            return x, result, {}
```

**关键点的后处理：**

+ maskrcnn_benchmark/modeling/roi_heads/keypoint_head/inference.py

```python
def make_roi_keypoint_post_processor(cfg):
    keypointer = Keypointer()
    keypoint_post_processor = KeypointPostProcessor(keypointer)
    return keypoint_post_processor
```



```python
class Keypointer(object):
    """
    Projects a set of masks in an image on the locations
    specified by the bounding boxes
    """

    def __init__(self, padding=0):
        self.padding = padding

    def __call__(self, masks, boxes):
        # TODO do this properly
        if isinstance(boxes, BoxList):
            boxes = [boxes]
        assert len(boxes) == 1

        result, scores = heatmaps_to_keypoints(
            masks.detach().cpu().numpy(), boxes[0].bbox.cpu().numpy()
        )
        return torch.from_numpy(result).to(masks.device), torch.as_tensor(scores, device=masks.device)
```



```python
def heatmaps_to_keypoints(maps, rois):
    """Extract predicted keypoint locations from heatmaps. Output has shape
    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)
    for each keypoint.
    """
    # This function converts a discrete image coordinate in a HEATMAP_SIZE x
    # HEATMAP_SIZE image to a continuous keypoint coordinate. We maintain
    # consistency with keypoints_to_heatmap_labels by using the conversion from
    # Heckbert 1990: c = d + 0.5, where d is a discrete coordinate and c is a
    # continuous coordinate.
    offset_x = rois[:, 0]
    offset_y = rois[:, 1]

    widths = rois[:, 2] - rois[:, 0]
    heights = rois[:, 3] - rois[:, 1]
    widths = np.maximum(widths, 1)
    heights = np.maximum(heights, 1)
    widths_ceil = np.ceil(widths)
    heights_ceil = np.ceil(heights)

    # NCHW to NHWC for use with OpenCV
    maps = np.transpose(maps, [0, 2, 3, 1])
    min_size = 0  # cfg.KRCNN.INFERENCE_MIN_SIZE
    num_keypoints = maps.shape[3]
    xy_preds = np.zeros((len(rois), 3, num_keypoints), dtype=np.float32)
    end_scores = np.zeros((len(rois), num_keypoints), dtype=np.float32)
    for i in range(len(rois)):
        if min_size > 0:
            roi_map_width = int(np.maximum(widths_ceil[i], min_size))
            roi_map_height = int(np.maximum(heights_ceil[i], min_size))
        else:
            roi_map_width = widths_ceil[i]
            roi_map_height = heights_ceil[i]
        width_correction = widths[i] / roi_map_width
        height_correction = heights[i] / roi_map_height
        roi_map = cv2.resize(
            maps[i], (roi_map_width, roi_map_height), interpolation=cv2.INTER_CUBIC
        )
        # Bring back to CHW
        roi_map = np.transpose(roi_map, [2, 0, 1])
        # roi_map_probs = scores_to_probs(roi_map.copy())
        w = roi_map.shape[2]
        pos = roi_map.reshape(num_keypoints, -1).argmax(axis=1)  ##根据heatmap的最大值求x, y的坐标
        x_int = pos % w
        y_int = (pos - x_int) // w
        # assert (roi_map_probs[k, y_int, x_int] ==
        #         roi_map_probs[k, :, :].max())
        x = (x_int + 0.5) * width_correction
        y = (y_int + 0.5) * height_correction
        xy_preds[i, 0, :] = x + offset_x[i]
        xy_preds[i, 1, :] = y + offset_y[i]
        xy_preds[i, 2, :] = 1
        end_scores[i, :] = roi_map[np.arange(num_keypoints), y_int, x_int]

    return np.transpose(xy_preds, [0, 2, 1]), end_scores
```



```python
class KeypointPostProcessor(nn.Module):
    def __init__(self, keypointer=None):
        super(KeypointPostProcessor, self).__init__()
        self.keypointer = keypointer

    def forward(self, x, boxes):
        mask_prob = x

        scores = None
        if self.keypointer:
            mask_prob, scores = self.keypointer(x, boxes)

        assert len(boxes) == 1, "Only non-batched inference supported for now"
        boxes_per_image = [box.bbox.size(0) for box in boxes]
        mask_prob = mask_prob.split(boxes_per_image, dim=0)
        scores = scores.split(boxes_per_image, dim=0)

        results = []
        for prob, box, score in zip(mask_prob, boxes, scores):
            bbox = BoxList(box.bbox, box.size, mode="xyxy")
            for field in box.fields():
                bbox.add_field(field, box.get_field(field))
            prob = PersonKeypoints(prob, box.size)
            prob.add_field("logits", score)
            bbox.add_field("keypoints", prob)
            results.append(bbox)

        return results
```

+ maskrcnn_benchmark/structures/keypoint.py

```python
class Keypoints(object):
    def __init__(self, keypoints, size, mode=None):
        # FIXME remove check once we have better integration with device
        # in my version this would consistently return a CPU tensor
        device = keypoints.device if isinstance(keypoints, torch.Tensor) else torch.device('cpu')
        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)
        num_keypoints = keypoints.shape[0]
        if num_keypoints:
            keypoints = keypoints.view(num_keypoints, -1, 3)
        
        # TODO should I split them?
        # self.visibility = keypoints[..., 2]
        self.keypoints = keypoints# [..., :2]

        self.size = size
        self.mode = mode
        self.extra_fields = {}

    def crop(self, box):
        raise NotImplementedError()

    def resize(self, size, *args, **kwargs):
        ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(size, self.size))
        ratio_w, ratio_h = ratios
        resized_data = self.keypoints.clone()
        resized_data[..., 0] *= ratio_w
        resized_data[..., 1] *= ratio_h
        keypoints = type(self)(resized_data, size, self.mode)
        for k, v in self.extra_fields.items():
            keypoints.add_field(k, v)
        return keypoints

    def transpose(self, method):
        if method not in (FLIP_LEFT_RIGHT,):
            raise NotImplementedError(
                    "Only FLIP_LEFT_RIGHT implemented")

        flip_inds = type(self).FLIP_INDS
        flipped_data = self.keypoints[:, flip_inds]
        width = self.size[0]
        TO_REMOVE = 1
        # Flip x coordinates
        flipped_data[..., 0] = width - flipped_data[..., 0] - TO_REMOVE

        # Maintain COCO convention that if visibility == 0, then x, y = 0
        inds = flipped_data[..., 2] == 0
        flipped_data[inds] = 0

        keypoints = type(self)(flipped_data, self.size, self.mode)
        for k, v in self.extra_fields.items():
            keypoints.add_field(k, v)
        return keypoints

    def to(self, *args, **kwargs):
        keypoints = type(self)(self.keypoints.to(*args, **kwargs), self.size, self.mode)
        for k, v in self.extra_fields.items():
            if hasattr(v, "to"):
                v = v.to(*args, **kwargs)
            keypoints.add_field(k, v)
        return keypoints

    def __getitem__(self, item):
        keypoints = type(self)(self.keypoints[item], self.size, self.mode)
        for k, v in self.extra_fields.items():
            keypoints.add_field(k, v[item])
        return keypoints

    def add_field(self, field, field_data):
        self.extra_fields[field] = field_data

    def get_field(self, field):
        return self.extra_fields[field]

    def __repr__(self):
        s = self.__class__.__name__ + '('
        s += 'num_instances={}, '.format(len(self.keypoints))
        s += 'image_width={}, '.format(self.size[0])
        s += 'image_height={})'.format(self.size[1])
        return s
```



数据增强：rotate 180度



e2e模型优化：

+ 数据增强；
+ 利用mmpose中的新算法；















maskrcnn中nms换成soft_nms







20210210，detectron2

```
[02/10 21:50:05 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[02/10 21:50:08 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 2.13 seconds.
[02/10 21:50:08 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/10 21:50:08 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.58 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.515
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.196
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.306
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.407
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.475
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.555
[02/10 21:50:08 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 24.359 | 51.510 | 19.576 | 11.814 | 30.614 | 37.775 |
Loading and preparing results...
DONE (t=0.14s)
creating index...
index created!
[02/10 21:50:08 d2.evaluation.fast_eval_api]: Evaluate annotation type *keypoints*
[02/10 21:50:19 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 10.95 seconds.
[02/10 21:50:19 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[02/10 21:50:19 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.14 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.277
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.556
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.239
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.384
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.694
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.362
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.478
[02/10 21:50:20 d2.evaluation.coco_evaluation]: Evaluation results for keypoints: 
|   AP   |  AP50  |  AP75  |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|
| 27.713 | 55.604 | 23.866 | 23.685 | 34.276 |
[02/10 21:50:20 d2.engine.defaults]: Evaluation results for keypoints_coco_2017_val in csv format:
[02/10 21:50:20 d2.evaluation.testing]: copypaste: Task: bbox
[02/10 21:50:20 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[02/10 21:50:20 d2.evaluation.testing]: copypaste: 24.3592,51.5099,19.5756,11.8136,30.6141,37.7746
[02/10 21:50:20 d2.evaluation.testing]: copypaste: Task: keypoints
[02/10 21:50:20 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APm,APl
[02/10 21:50:20 d2.evaluation.testing]: copypaste: 27.7133,55.6039,23.8656,23.6851,34.2756
```



20210305，maskrcnn-benchmark + hrnet

```
2021-03-06 20:15:29,870 maskrcnn_benchmark.inference INFO: Preparing results for COCO format
2021-03-06 20:15:29,871 maskrcnn_benchmark.inference INFO: Preparing bbox results
2021-03-06 20:15:30,958 maskrcnn_benchmark.inference INFO: Preparing keypoints results
2021-03-06 20:15:32,812 maskrcnn_benchmark.inference INFO: Evaluating predictions
Loading and preparing results...
DONE (t=1.71s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=16.39s).
Accumulating evaluation results...
DONE (t=1.96s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.295
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.599
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.190
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.125
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.352
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.551
Loading and preparing results...
DONE (t=6.68s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *keypoints*
DONE (t=15.68s).
Accumulating evaluation results...
DONE (t=0.56s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.343
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.629
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.327
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.319
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.390
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.434
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.728
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.430
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.391
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.492
2021-03-06 20:16:28,678 maskrcnn_benchmark.inference INFO: 
Task: bbox
AP, AP50, AP75, APs, APm, APl
0.2951, 0.5992, 0.2566, 0.1897, 0.3627, 0.3958
Task: keypoints
AP, AP50, AP75, APm, APl
0.3429, 0.6286, 0.3272, 0.3193, 0.3897
```



20210305，maskrcnn-benchmark + hrnet + heatmap(1000)

```
2021-03-06 20:17:33,639 maskrcnn_benchmark.inference INFO: Preparing results for COCO format
2021-03-06 20:17:33,639 maskrcnn_benchmark.inference INFO: Preparing bbox results
2021-03-06 20:17:34,685 maskrcnn_benchmark.inference INFO: Preparing keypoints results
2021-03-06 20:17:36,475 maskrcnn_benchmark.inference INFO: Evaluating predictions
Loading and preparing results...
DONE (t=1.43s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=15.62s).
Accumulating evaluation results...
DONE (t=1.67s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.277
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.573
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.179
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.335
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.120
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541
Loading and preparing results...
DONE (t=6.21s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *keypoints*
DONE (t=15.06s).
Accumulating evaluation results...
DONE (t=0.48s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.329
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.619
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.313
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.302
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.381
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.420
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.727
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.419
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.375
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.483
2021-03-06 20:18:29,287 maskrcnn_benchmark.inference INFO: 
Task: bbox
AP, AP50, AP75, APs, APm, APl
0.2770, 0.5732, 0.2314, 0.1786, 0.3354, 0.3755
Task: keypoints
AP, AP50, AP75, APm, APl
0.3288, 0.6186, 0.3127, 0.3016, 0.3806
```



20210309，maskrcnn-benchmark + hrnet + heatmap(2000)

```
Evaluate annotation type *bbox*
DONE (t=15.65s).
Accumulating evaluation results...
DONE (t=1.62s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.586
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.246
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.179
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.122
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.432
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.492
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.552
Loading and preparing results...
DONE (t=6.09s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *keypoints*
DONE (t=14.09s).
Accumulating evaluation results...
DONE (t=0.49s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.372
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.651
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.341
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.432
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.465
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.759
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.486
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.416
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.531
2021-03-10 02:39:44,233 maskrcnn_benchmark.inference INFO: 
Task: bbox
AP, AP50, AP75, APs, APm, APl
0.2858, 0.5863, 0.2462, 0.1790, 0.3459, 0.3905
Task: keypoints
AP, AP50, AP75, APm, APl
0.3724, 0.6507, 0.3796, 0.3414, 0.4321
```



20210310，maskrcnn-benchmark + hrnet + heatmap(3000)

```
Evaluate annotation type *bbox*
DONE (t=15.04s).
Accumulating evaluation results...
DONE (t=1.62s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.289
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.592
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.244
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.184
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.122
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.432
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.489
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549
Loading and preparing results...
DONE (t=6.09s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *keypoints*
DONE (t=14.27s).
Accumulating evaluation results...
DONE (t=0.52s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.390
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.667
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.405
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.357
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.455
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.482
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.772
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.509
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.431
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.552
2021-03-11 03:05:40,319 maskrcnn_benchmark.inference INFO: 
Task: bbox
AP, AP50, AP75, APs, APm, APl
0.2886, 0.5923, 0.2445, 0.1845, 0.3479, 0.3886
Task: keypoints
AP, AP50, AP75, APm, APl
0.3902, 0.6672, 0.4055, 0.3575, 0.4548
```





20210312，maskrcnn-benchmark + hrnet + heatmap(10000)

```
Evaluate annotation type *bbox*
DONE (t=16.96s).
Accumulating evaluation results...
DONE (t=2.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.481
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.293
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.101
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Loading and preparing results...
DONE (t=9.02s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *keypoints*
DONE (t=14.61s).
Accumulating evaluation results...
DONE (t=0.62s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.330
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.579
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.334
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.299
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.397
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.451
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.736
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.469
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.391
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.532
2021-03-13 23:49:41,054 maskrcnn_benchmark.inference INFO: 
Task: bbox
AP, AP50, AP75, APs, APm, APl
0.2106, 0.4809, 0.1500, 0.1289, 0.2581, 0.2925
Task: keypoints
AP, AP50, AP75, APm, APl
0.3301, 0.5793, 0.3337, 0.2993, 0.3975
```

